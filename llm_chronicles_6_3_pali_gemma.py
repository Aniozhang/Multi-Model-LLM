# -*- coding: utf-8 -*-
"""llm-chronicles-6.3-pali-gemma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wEkBQcYq8-xsyGlvgxpXQxjP_yN9F0FU

# 1 - Multi-Modal LLM: PaliGemma

The PaliGemma model is a multi-modal large language model (MLLM) capable of processing various input modalities (image, audio, video) to generate text.

Notebooked based on:
- https://colab.research.google.com/drive/1gOhRCFyt9yIoasJkd4VoaHcIqJPdJnlg?usp=sharing#scrollTo=zchyD7YIsmi7
"""

!pip install -q -U accelerate bitsandbytes transformers

from huggingface_hub import notebook_login

notebook_login()

import torch
from transformers import AutoTokenizer, PaliGemmaForConditionalGeneration, PaliGemmaProcessor

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_id = "google/paligemma-3b-mix-224"
model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16)
model.to(device)

processor = PaliGemmaProcessor.from_pretrained(model_id)

"""## 1.1 - Model Architecture

The model architecture with its core components is summarized here:

![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/6.3/multi-modal-llm.png)


- **Modality Encoder**:
Processes different input data types into a unified representation. For vision, it uses the SiglipVisionModel, which includes embeddings to convert images into patch embeddings, and an encoder with multiple layers of self-attention and MLP.

- **Modality Interface**:
Projects the encoded data into a format suitable for the language model using the PaliGemmaMultiModalProjector, a linear layer that converts encoded modalities to match the language model's input dimensions.

- **Large Language Model (LLM)**:
Generates text based on the unified representation. The GemmaForCausalLM language model includes an embedding layer for text tokens, multiple decoder layers with self-attention and MLP, and an LM head to output the final text predictions.


Vision Transformer

![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/6.3/vision-transformer.png)


"""

print(model)

inputs = processor(text="Describe the image", images=peacock_picture,
                  padding="longest", do_convert_rgb=True, return_tensors="pt").to("cuda")
print(inputs)

"""# 2 - Inference"""

import torch
import numpy as np
from PIL import Image
import requests
import matplotlib.pyplot as plt

def load_image(url):
  input_image = Image.open(requests.get(url, stream=True).raw)
  plt.imshow(input_image)
  plt.axis('off')  # Turn off axis numbers and ticks
  plt.show()
  return input_image

peacock_picture = load_image("https://static.vecteezy.com/system/resources/previews/032/257/185/non_2x/wallpapers-for-the-beautiful-peacock-wallpaper-ai-generated-free-photo.jpg")

def query(image, prompt):
  inputs = processor(text=prompt, images=image,
                  padding="longest", do_convert_rgb=True, return_tensors="pt").to("cuda")
  inputs = inputs.to(dtype=model.dtype)

  with torch.no_grad():
    output = model.generate(**inputs, max_length=496)

  return processor.decode(output[0], skip_special_tokens=True)

"""## 2.1 - Image description"""

print(query(peacock_picture, "Describe the image"))

"""## 2.2 - Visual Q&A"""

print(query(peacock_picture, "What color is the peacock?"))

print(query(peacock_picture, "What is the peacock standing on?"))

print(query(peacock_picture, "What color are the flowers?"))

"""## 2.3 - OCR"""

receipt = load_image("https://i.redd.it/sa4bzhkgewj81.jpg")

print(query(receipt, "What is this?"))

print(query(receipt, "What did the customer buy?"))

print(query(receipt, "What is the total?"))

print(query(receipt, "How many bananas did the customer buy?"))

book = load_image("https://images.pangobooks.com/images/337e7e87-b26e-42d8-b4be-57a4825c39b2?quality=85&width=1200&crop=1%3A1")

print(query(book, "What is this?"))

print(query(book, "Who is the author?"))

print(query(book, "What's the title?"))

"""## 2.4 - Object detection"""

output = query(peacock_picture, "detect peacock")
print(output)

import re
from PIL import Image, ImageDraw
import numpy as np
import matplotlib.pyplot as plt

def display_bounding_box(pil_image, model_output):
    # Parse the location tokens
    loc_values = re.findall(r'<loc(\d+)>', model_output)
    loc_values = [int(val) for val in loc_values]

    # Convert normalized coordinates to image coordinates
    width, height = pil_image.size
    y_min, x_min, y_max, x_max = [
        int(loc_values[i] / 1024 * (height if i % 2 == 0 else width))
        for i in range(4)
    ]

    # Create a copy of the image to draw on
    draw_image = pil_image.copy()
    draw = ImageDraw.Draw(draw_image)

    # Draw the bounding box
    draw.rectangle([x_min, y_min, x_max, y_max], outline="red", width=3)

    # Display the image
    plt.figure(figsize=(10, 10))
    plt.imshow(draw_image)
    plt.axis('off')
    plt.show()

display_bounding_box(peacock_picture, output)

